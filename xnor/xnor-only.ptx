//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21124049
// Cuda compilation tools, release 8.0, V8.0.44
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_50
.address_size 64

	// .globl	Xnor

.visible .entry Xnor(
	.param .u64 _Z14XnorPiS__param_0,
	.param .u64 _Z14XnorPiS__param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<106>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd2, [_Z14XnorPiS__param_0];
	ld.param.u64 	%rd3, [_Z14XnorPiS__param_1];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r8, %tid.x;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r8, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r1, [%rd7];
	add.s64 	%rd1, %rd4, %rd6;
	ld.global.u32 	%r105, [%rd1];
	mov.u32 	%r104, -16777216;

BB0_1:
	xor.b32  	%r9, %r105, %r1;
    not.b32     %r9, %r9;
	xor.b32  	%r12, %r9, %r1;
    not.b32     %r12, %r12;
	xor.b32  	%r15, %r12, %r1;
    not.b32     %r15, %r15;
	xor.b32  	%r18, %r15, %r1;
    not.b32     %r18, %r18;
	xor.b32  	%r21, %r18, %r1;
    not.b32     %r21, %r21;
	xor.b32  	%r24, %r21, %r1;
    not.b32     %r24, %r24;
	xor.b32  	%r27, %r24, %r1;
    not.b32     %r27, %r27;
	xor.b32  	%r30, %r27, %r1;
    not.b32     %r30, %r30;
	xor.b32  	%r33, %r30, %r1;
    not.b32     %r33, %r33;
	xor.b32  	%r36, %r33, %r1;
    not.b32     %r36, %r36;
	xor.b32  	%r39, %r36, %r1;
    not.b32     %r39, %r39;
	xor.b32  	%r42, %r39, %r1;
    not.b32     %r42, %r42;
	xor.b32  	%r45, %r42, %r1;
    not.b32     %r45, %r45;
	xor.b32  	%r48, %r45, %r1;
    not.b32     %r48, %r48;
	xor.b32  	%r51, %r48, %r1;
    not.b32     %r51, %r51;
	xor.b32  	%r54, %r51, %r1;
    not.b32     %r54, %r54;
	xor.b32  	%r57, %r54, %r1;
    not.b32     %r57, %r57;
	xor.b32  	%r60, %r57, %r1;
    not.b32     %r60, %r60;
	xor.b32  	%r63, %r60, %r1;
    not.b32     %r63, %r63;
	xor.b32  	%r66, %r63, %r1;
    not.b32     %r66, %r66;
	xor.b32  	%r69, %r66, %r1;
    not.b32     %r69, %r69;
	xor.b32  	%r72, %r69, %r1;
    not.b32     %r72, %r72;
	xor.b32  	%r75, %r72, %r1;
    not.b32     %r75, %r75;
	xor.b32  	%r78, %r75, %r1;
    not.b32     %r78, %r78;
	xor.b32  	%r81, %r78, %r1;
    not.b32     %r81, %r81;
	xor.b32  	%r84, %r81, %r1;
    not.b32     %r84, %r84;
	xor.b32  	%r87, %r84, %r1;
    not.b32     %r87, %r87;
	xor.b32  	%r90, %r87, %r1;
    not.b32     %r90, %r90;
	xor.b32  	%r93, %r90, %r1;
    not.b32     %r93, %r93;
	xor.b32  	%r96, %r93, %r1;
    not.b32     %r96, %r96;
	xor.b32  	%r99, %r96, %r1;
    not.b32     %r99, %r99;
	xor.b32  	%r102, %r99, %r1;
    not.b32     %r102, %r102;
	add.s32 	%r104, %r104, 32;
	setp.ne.s32	%p1, %r104, 0;
	@%p1 bra 	BB0_1;

	st.global.u32 	[%rd1], %r102;
	ret;
}


